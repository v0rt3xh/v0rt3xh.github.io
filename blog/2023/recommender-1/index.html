<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Recommender System (1) YouTube Recommender | Haitao Huang </title> <meta name="author" content="Haitao Huang "> <meta name="description" content="The first post of the Recommender Series."> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%8C%90&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://v0rt3xh.github.io/blog/2023/recommender-1/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> </head> <body> <d-front-matter> <script async type="text/json">{
      "title": "Recommender System (1) YouTube Recommender",
      "description": "The first post of the Recommender Series.",
      "published": "January 27, 2023",
      "authors": [
        {
          "author": "Haitao Huang",
          "authorURL": "",
          "affiliations": [
            {
              "name": "",
              "url": ""
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Haitao Huang </span></a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Posts<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Recommender System (1) YouTube Recommender</h1> <p>The first post of the Recommender Series.</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div><a href="#introduction">Introduction</a></div> <div><a href="#system-overview">System Overview</a></div> <div><a href="#design-details">Design Details</a></div> <ul> <li><a href="#candidate-generation">Candidate Generation</a></li> <li><a href="#ranking">Ranking</a></li> </ul> </nav> </d-contents> <h2 id="introduction">Introduction</h2> <p>Many of us have been using YouTube frequently. On the homepage, we can always find some videos to watch. I often watch cooking recipes and sports highlights. As a result, YouTube usually presents food or sports channels to me. Sometimes, I can also find some trending videos. So, how did YouTube’s recommender system work? In this post, I will revisit a classic paper that introduces the structure of the YouTube recommender system: Deep Neural Networks for YouTube Recommendations<d-cite key="covington2016deep"></d-cite>.</p> <p>From the perspective of information retrieval, a recommender system consists of two major components: the <em>candidate generation model</em> and the <em>ranking model</em>. In the example of YouTube, the recommender model is expected to select hundreds of candidate videos from the massive corpus and assign scores to candidates for ranking.</p> <div class="caption"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/Dichotomy-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/Dichotomy-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/Dichotomy-1400.webp"></source> <img src="/assets/img/Dichotomy.png" class="img-fluid rounded z-depth-1" width="360" height="auto" title="dichotomy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="caption"> An illustration of candidate generation and ranking, assuming that we need 26 candidates. </div> <p>In the paper, the authors list three challenges of building such recommender model for YouTube.</p> <ul> <li> <p><strong>Scale</strong>: YouTube has a massive user base and video corpus, which demands highly specialized distributed learning algorithms and efficient serving systems.</p> <p>A recent estimation shows that there are more than 122 million daily active users on YouTube (<a href="https://www.demandsage.com/youtube-stats/" rel="external nofollow noopener" target="_blank">source</a>). To recommend videos to those users, the inference method must be efficient to meet latency requirements. On the other hand, active users interact with different videos every day and produce new data. We can improve the model with those data. In this case, training the model on a single node might not be feasible due to the data size. Thus, distributed learning algorithms are preferable. <d-footnote>TODO: Will write a post about distributed learning later.</d-footnote></p> </li> <li> <p><strong>Freshness</strong>: The system should be responsive to recent uploaded content as well as the latest action taken by the users.</p> <p>There exists a trade-off between new content and well-established content. Without careful design, the recommender could keep presenting vintage videos (e.g. uploaded 8 years ago, 1M+ views, 10K+ thumbs up) to users. Content creators might find it hard to make their recent works stand out. Also, if a user searches for a new topic, the recommender should include relevant videos in the next impression.</p> </li> <li> <p><strong>Noise</strong>: In training data, only noisy implicit feedback signals are available. Ground truth of user satisfication is rarely available.</p> <p>On many occasions, lots of YouTube users do not click the “like” button even if they enjoy the video. The same goes for the “dislike” button. In this sense, relying on explicit feedback will introduce sparse training data. Therefore, developers and managers often need to monitor metrics like click-through rate, video watch time, etc.</p> </li> </ul> <h2 id="system-overview">System Overview</h2> <p>The following figure shows the overall structure of the YouTube recommender system.</p> <div class="caption"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/YoutubeArc-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/YoutubeArc-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/YoutubeArc-1400.webp"></source> <img src="/assets/img/YoutubeArc.png" class="img-fluid rounded z-depth-1" width="360" height="auto" title="dichotomy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <p>As mentioned before, the system consists of the candidate generation network as well as the ranking network. The candidate generation network takes the events from users’ YouTube activity history as input and retrieves a small subset (hundreds) of videos from the large video corpus (millions or more). The ranking network takes in a rich set of features describing the video and user and assigns a score to each video according to an objective function. Highest-scoring videos are presented to the user.</p> <p>During development, offline metrics (precision, recall, ranking loss, etc.) are used to guide iterative improvements to the system. In production, to determine the effectiveness of an algorithm/model, the authors rely on A/B testing via live experiments. In the experiment, one can monitor click-through rate, watch time, and other metrics that measure user engagement. One interesting observation is that <em>live A/B results are not always correlated with offline experiments.</em> <d-footnote>TODO: Will write a post about A/B testing later.</d-footnote></p> <h2 id="design-details">Design Details</h2> <ul> <li> <p><strong>Candidate generation network</strong></p> <p>Take events from the user’s YouTube activity history as input and retrieve a small subset of videos (hundreds) from a large corpus (millions). The objective of this network is to get highly relevant (in the measure of precision) candidates for the user.</p> <p>This network only provides broad personalization via collaborative filtering. The similarity between users is expressed in terms of coarse features such as IDs of watched videos, search query tokens, and demographics.</p> </li> <li> <p><strong>Ranking network</strong></p> <p>Its objective is to get a fine-level representation of the candidates and present a few “best” recommendations in a list. The importance of candidates should be assigned with high recall. The ranking network takes in a rich set of features (describing the video and user) and assigns a score to each video according to an objective function. Highest scoring videos are presented to the user. </p> </li> </ul> <h2 id="candidate-generation">Candidate Generation</h2> <p>The generation process finds hundreds of videos that may be relevant to users from the enormous corpus. Previously, YouTube was using a matrix factorization approach trained under rank loss<d-cite key="weston2011wsabie"></d-cite>. The new approach in this paper can be viewed as a non-linear generalization of the factorization technique during its early iterations: shallow networks that embed users’ previous watches are used. (Only utilize user-item/user-video information.)</p> <h3 id="recommendation-as-classification">Recommendation as Classification</h3> <p>There are many ways to formulate recommendations. Most of the time, we are predicting whether a positive outcome would happen. For instance, we can define the positive outcome for Youtube recommendation as “User clicks the suggested video”. Then, our objective is to predict the probability of that event.</p> <p>The authors pose recommendation as an extreme multiclass classification problem. The problem is to accurately predict a specifi video watch $w_t$ at time $t$ among millions of videos $i$ from a corpus $V$ based on a user $U$ and context $C$,</p> \[P(w_t = i | U, C) = \frac{e^{v_iu}}{\sum_{j \in V} e^{v_ju}}\] <p>where $u \in R^N$ is a high-dimensional embedding of the user, context pair. $v_j \in R^N$ is the embedding of a candidate video. The task of the neural network is to learn user embedding $u$ as a function of the user’s history and context. In this setting, a positive outcome or a positive training example is “a user completing a video”.</p> <p>There are millions of classes in this problem. If we use all the classes, the softmax layer would be very large and computing the normalization term becomes the bottleneck. The authors relies on a technique to sample negative classes from the background distribution (candidate sampling) and then correct for this sampling via importance weighting <d-cite key="jean2014using"></d-cite>.</p> <hr> <p><strong>Quick Intro to The Sampling Method Above</strong></p> <p>The sampling method is originally used under a neural machine translation setting, where there is a large vocabulary set. Prior to training, we partition the training corpus and define a subset \(V^{'}\) of the target vocabulary for each partition. We accumulate unique target words in the training sentences until a predefined threshold $\tau$ is reached.</p> <p>Then, for partition $i$ with word subset \(V^{'}_{i}\), we can define the following proposal distribution \(Q_{i}\). \(Q_{i}\) assigns equal probability mass to the words in \(V^{'}_{i}\) and zero probability mass to all other words.</p> <p>Replacing “words” by “videos”, we can derive the sampling method used in the YouTube candidate generation network. For each training example, the cross-entropy loss is minimized for the true label and negative samples within this training partition. In practice, several thousands negatives are sampled.</p> <hr> <p>At serving time, one need to compute the most likely K classes (videos) to present them to the user. The latency requirement could be tens of milliseconds. An approximate scoring scheme sublinear in the number of classes is needed. Previous systems at YouTube relied on Hashing, which is adopted here as well.</p> <hr> <p><strong>Quick Intro to The Hashing Method</strong></p> <ol> <li>Given a user history and context \(x^{*}\), we use a input partitioner to map it to a set of partitions \(p = g(x^{*})\).</li> <li>We retrieve the label sets assigned to each partition \(p_i \in p\). Here, a label is a video. Taking the union of those sets, we have the selected label set: \(L = \cup _{i = 1}^{\|p\|} L_{p_{i}}\), where \(L_{p_{i}}\) is the subset of labels assigned to partition \(p_{i}\).</li> <li>At last, we score the labels (videos) \(y \in L\) with the candidate generation network and rank them to produce results.</li> </ol> <p>The input partitioner aims to optimize precision at top \(N\). For more details, please refer to the original paper<d-cite key="weston2013label"></d-cite>.</p> <hr> <p>In the inference stage, calibrated likelihoods from the softmax layer are not needed. So the problem reduces to a nearest neighbor search problem in the dot product space. To score and rank the candidates, we can use many nearest neighbor search algorithms. The authors claim that A/B test results were not particularly sensitive to the choice of the search algorithm.</p> <h3 id="model-architecture">Model Architecture</h3> <p>Motivated by the continuous bag of words language models, the authors learn high dimensional embeddings for each video in a fixed vocabulary and feed those embeddings into a feedforward neural network. A user’s watch history is represented by a variable-length sequence of sparse video IDs which is mapped to a dense vector representation via the embeddings. Among the mapping strategies (sum, component-wise max, etc.), taking the average performs the best. The embeddings are learned jointly with all other parameters in the network. Features are concatenated into a wide first layer, followed by several layers of fully connected ReLU.</p> <div class="caption"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/candidateStruct-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/candidateStruct-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/candidateStruct-1400.webp"></source> <img src="/assets/img/candidateStruct.png" class="img-fluid rounded z-depth-1" width="auto" height="400" title="canStruct" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="caption"> The structure of the candidate generation network. </div> <h3 id="input-features">Input Features</h3> <p>Demographic features are important for providing priors so that we have reasonable recommendations to new users. The geographic region and device features are embedded and concatenated. Simple binary and continuous features (gender, logged-in state, and age) are input directly into the network, normalized into \([0, 1]\).</p> <p><strong>“Example Age” Feature</strong></p> <p>In the figure above, you may have noticed that there is a feature named “example age” in the input layer. This feature is introduced to alleviate an underlying issue: a machine learning model tends to exhibit an implicit bias towards the past, as it’s trained from historical examples to predict future results.</p> <p>YouTube users prefer fresh content, but not at the expense of relevance. Recommending recently uploaded content to users is important. One challenge is that the distribution of video popularity is highly non-stationary. But the multimodal distribution over the corpus produced by our recommender will reflect the average watch likelihood in the training window of several weeks. In other words, without supplementary signals, the recommender may not recognize the potential popularity of fresh content.</p> <p>To correct this, the authors feed the age of the training example as a feature during training. At serving time, the age is set to zero (or slightly negative) to reflect that the model is making predictions at the very end of the training window. The figure below shows the effectiveness of this method.</p> <div class="caption"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/exampleAge-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/exampleAge-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/exampleAge-1400.webp"></source> <img src="/assets/img/exampleAge.png" class="img-fluid rounded z-depth-1" width="auto" height="400" title="Age" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="caption"> Without the example age feature, the model will predict the average likelihood over the training window (Blue line). </div> <h3 id="label-and-context-selection">Label and Context Selection</h3> <p>Training examples are generated using all YouTube watches (even those embedded on other sites) rather than just watches on the recommendations the authors produce. Otherwise, it would be hard for new content to surface and the recommender would be overly biased toward exploitation.</p> <p>Another insight is that generating a fixed number of training examples per user improves live metrics (effectively weighting users equally in the loss function). This prevents highly active users from dominating the loss.</p> <p>Natural consumption patterns of videos typically lead to very asymmetric co-watch probabilities. Episodic series are usually watched sequentially and users often discover artists in a genre beginning with the most broadly popular before focusing on smaller niches. The authors find much better performance predicting the user’s next match, rather than predicting a randomly held-out watch.</p> <p>Many collaborative filtering systems implicitly choose the labels and context by holding out a random item and predicting it from other items in the user’s history. This leaks future information and ignores any asymmetric consumption patterns. In contrast, the authors rollback a user’s history by choosing a random watch and only input actions the user took before the held-out label watch. The following figure shows the difference between them.</p> <div class="caption"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/predictWatch-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/predictWatch-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/predictWatch-1400.webp"></source> <img src="/assets/img/predictWatch.png" class="img-fluid rounded z-depth-1" width="auto" height="400" title="Watch" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <h2 id="ranking">Ranking</h2> <p>Now, we can use impression data to specialize and calibrate candidates for the particular user interface. For instance, a user may watch a given video with high probability generally but is unlikely to click the homepage impression due to the thumbnail image.</p> <p>During ranking, we have access to many more features describing the video and the user’s relationship to the video. Ranking is also important for ensembling different candidate sources of which scores are not directly comparable.</p> <p>The ranking network architecture is similar to the candidate generation network. It assigns an independent score to each video impression using logistic regression. The list of videos is sorted by the score and returned to the user. The final ranking objective is constantly being tuned based on live A/B testing results. But it’s generally a simple function of expected watch time per impression.</p> <blockquote> <p>Ranking by click-through rate often promotes click-bait videos. Watch time can better capture user engagement.</p> </blockquote> <p>The architecture can be represented as below.</p> <div class="caption"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/rankingArc-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/rankingArc-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/rankingArc-1400.webp"></source> <img src="/assets/img/rankingArc.png" class="img-fluid rounded z-depth-1" width="auto" height="400" title="Rank" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <h3 id="feature-rrepresentation">Feature Rrepresentation</h3> <p>The features of YouTube have some interesting properties:</p> <ul> <li>Categorical features may vary widely in their cardinality: some are binary (logged-in status), some has millions of values (users’ last search queries.). Features are further split according to whether they contribute to only a single value (“univalent”) or a set of values (“multivalent”). <blockquote> <p>Example of a univalent categorical feature is the video ID of the impression being scored, while a corresponding multivalent feature might be a bag of the last 10 video IDs the user has watched.</p> </blockquote> </li> <li>Features can also be classified according to whether they describe properties of the item (“impression”) or the properties of the user/context (“query”). Query features are computed once per request while impression features are computed for each item scored.</li> </ul> <h3 id="feature-engineering">Feature Engineering</h3> <p>Typically, hundreds of features are used in the ranking model, roughly split evenly between categorical and continuous features. The main challenge is in representing a temporal sequence of user actions and how these actions relate to the video impression being scored. The most important signals are those that describe a user’s previous interaction with the item itself and other similar items.</p> <blockquote> <p>Consider the user’s past history with the channel that uploaded the video being scored - how many videos has the user watched from the channel? When was the last time the user watched a video in this topic?</p> </blockquote> <p>These continuous features describing past user actions on related items are particularly powerful because they generalize well across disparate items. The authors also find it crucial to propagate information from candidate generation into ranking in the form of features, e.g. which sources nominated this video candidate? What scores did they assign?</p> <p>Features describing the frequency of past video impressions are critical for introducing “churn” in recommendations (successive requests do not return identical lists.). If a user was recently recommended a video but did not watch it then the model will naturally demote this impression on the next page load.</p> <h3 id="embedding-categorical-features">Embedding Categorical Features</h3> <p>For categorical features, each unique ID space (“vocabulary”) has a separately learned embedding with a dimension that increases approximately proportional to the logarithm of the number of unique values. Very large cardinality ID spaces are truncated by including only the top \(N\) after sorting based on their frequency in clicked impressions. Out-of-vocabulary values are simply mapped to zero embedding. Multivalent categorical features are averaged before feeding into the network.</p> <p>Importantly, categorical features in the same ID space also share underlying embeddings. For example, there exists a single global embedding of video IDs that many distinct features use: video ID of the impression, last video watched by the user, video ID that seeded the recommendation. <strong>Despite the shared embedding, each feature is fed separately into the network so that the layers above can learn specialized representations per feature</strong>.</p> <h3 id="normalizing-continuous-features">Normalizing Continuous Features</h3> <p>Proper normalization of continuous features was critical for convergence. Feature values that are equally distributed in (0,1) are desired. The authors used the following method: Assume there is a continuous feature \(x\) with distribution $f$, the normalized feature \(\tilde{x}\) can be expressed as</p> \[\tilde {x} = \int _{-\infty}^{x} df\] <p>One can approximate the above integral by linear interpolation on the quantiles of feature values.</p> <p>$x^2, x^{0.5}$ are included for more expressive power, since they can form super- and sub-linear functions of the feature. This could improve offline accuracy.</p> <h3 id="modeling-expected-watch-time">Modeling Expected Watch Time</h3> <p>The goal of the ranking network is to predict expected watch time given training examples that are either positive (the video impression was clicked) or negative (the impression was not clicked). The model is trained with weighted logistic regression under cross-entropy loss. The positive (clicked) impressions are weighted by the observed watch time on the video. Negative (unclicked) impressions all receive unit weight.</p> <p>Assuming that we have the probability for a user to click a video as \(p = \frac{k}{N}\), where \(k\) is the number of positive impressions. Now, the odds should look like this:</p> \[\frac{p}{1 - p} = \frac{k}{N - k}\] <p>With the weighting scheme above, the odds can be further expressed as:</p> \[\frac{k}{N - k} = \frac{\frac{1}{N}(T_{1} + T_{2} + ... + T_{K})}{1 - \frac{k}{N}} = \frac{E[T_{i}]}{1 - p} \approx E[T_{i}]\] <p>Where \(T_{i}\) is the watch time of the \(i^{th}\) positive impression. Since the click probability \(p\) is usually small on YouTube, the odds is an approximation to the expected watch time. Thus, when serving, we can use the exponential function \(e^{x}\) to produce scores, which closely estimates expected watch time.</p> <p>In this case, the loss function has the following form:</p> \[L = - \sum _{i = 1}^{N} (y_{i} w_{i} log(\hat{y}_{i}) + (1 - y_{i}) w_{i} log(1 - \hat{y}_{i}))\] <p>Where \(y_{i} = 1\) if the impression is positive, \(y_{i} = 0\) otherwise.</p> <p>\(w_{i} = T_{i}\) if the impression is positive, \(w_{i} = 1\) otherwise.</p> <p>But I haven’t figured out the connection between this loss function and previously deduced odds. If you have some thoughts, please feel free to comment below.</p> <h3 id="thanks-for-reading">Thanks for Reading!</h3> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/2023-01-27-distill.bib"></d-bibliography><div id="giscus_thread" style="max-width: 800px; margin: 0 auto;"> <script>let giscusTheme=localStorage.getItem("theme"),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"v0rt3xh/v0rt3xh.github.io","data-repo-id":"R_kgDOIzXPew","data-category":"General","data-category-id":"DIC_kwDOIzXPe84CTrXE","data-mapping":"title","data-strict":"0","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":giscusTheme,"data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,e])=>giscusScript.setAttribute(t,e)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2025 Haitao Huang . Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Last updated: January 11, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>